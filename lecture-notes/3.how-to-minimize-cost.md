# [How to minize cost](https://www.youtube.com/watch?v=TxIVr-nk1so)

간략한 설명을 위해 `H(x) = Wx + b`에서 `b`가 존재하지 않는 Simplified Hypothesis를 기준으로 설명을 한다.

<p align="center">
  <img src="/assets/3.how-to-minimize-cost/1.png?raw=true" alt="regression data table"/>
</p>

Cost를 최소화하는 것이 목표인데 시각적으로 봤을 때는 한눈에 들어온다. 하지만 이것을 기계적으로 바로 찾으려면 어떻게 해야할까?

## Gradient descent algorithm

- Minimize cost function
- Gradient descent is used many minimization problems
- For a vigen cost function, `cost(W, b)`, it will find W, b to minizite cost
- It can be applied to more general function: `cost(w1, w2, ...)`

경사하강법, 경사를 따라 내려가는 알고리즘

### How it works?

- Start with initial guesses
  - Start a 0,0 (or any other value)
  - Keeping changing W and b a little bit to try and reduce `cost(W, b)`
- Each time you change the parameters, you select the gradient which reduces `cost(W, b)` the most possible
- Repeat
- Do so until you converge to a local minimum
- Has an interesting property
  - Where you start can determine which minimum you end up

### 경사는 어떻게 구할까? (Formal definition)

미분이 사용된다.

<p align="center">
  <img src="/assets/3.how-to-minimize-cost/2.png?raw=true" alt="regression data table"/>
</p>

`1/2`의 경우 미분의 편리함을 위해 추가되었으며 최소값을 찾는 것은 실질적으로 같으므로 의미는 변하지 않는다.

<p align="center">
  <img src="/assets/3.how-to-minimize-cost/3.png?raw=true" alt="regression data table"/>
</p>

알파의 경우 learning rate라 부른다. 상수 0.1으로 가정하자. 어떤 점에서의 기울기를 구하고 그 기울기에 따라 이동 방법을 선택한다. 양의 기울기일 경우 다음 W 값은 현재보다 줄어들며 음의 기울기일 때는 W 값은 증가한다. 

아래는 미분 과정이다.

<p align="center">
  <img src="/assets/3.how-to-minimize-cost/4.png?raw=true" alt="regression data table"/>
</p>

마지막 식이 경사하강법에 대한 식을 의미한다.

<p align="center">
  <img src="/assets/3.how-to-minimize-cost/5.png?raw=true" alt="regression data table"/>
</p>

하지만 위 그림에서 왼쪽부터 시작해서 축을 각각 `cost`, `W`, `b`로 두고 볼 때 처음 값에 따라 결과값이 달라질 수 있다는 것을 알 수 있고 이것은 알고리즘이 잘 작동한다고 말할 수 없게된다.

<p align="center">
  <img src="/assets/3.how-to-minimize-cost/6.png?raw=true" alt="regression data table"/>
</p>

simplified hypothesis는 위 그림과 같이 나오게 된다. 어느 점에서 시작하던지 우리가 원하는 지점으로 수렴하게 된다. cost function이 설계할 때 `cost(W, b)`의 모양이 convex function이 되는지 확인이 필요하다.

# [ML lab 03 - Linear Regression의 cost 최소화의 TensorFlow 구현](https://www.youtube.com/watch?v=pHPmzTQ_e2o)

pyplot 사용을 위해 matplotlib을 설치해야한다.

```bash
$ pip3 install matplotlib
```

```python
import tensorflow as tf
import matplotlib.pyplot as plt

# tf Graph Input
X = [1., 2., 3.]
Y = [1., 2., 3.]
m = n_samples = len(X)

# Set model weights
W = tf.placeholder(tf.float32)

# Construct a linear model
hypothesis = tf.mul(X, W)  # H(x) = Wx

# Cost function
cost = tf.reduce_sum(tf.pow(hypothesis - Y, 2)) / m  # 1/m * (sigma(Wx_i - y_i)^2)

# Initializing the variables
init = tf.global_variables_initializer()

# For graphs
W_val = []
cost_val = []

# Launch the graph
sess = tf.Session()
sess.run(init)
for i in range(-30, 50):
    print(i * 0.1, sess.run(cost, feed_dict={W: i * 0.1}))
    W_val.append(i * 0.1)
    cost_val.append(sess.run(cost, feed_dict={W: i * 0.1}))

# Graphic display
plt.plot(W_val, cost_val, 'ro')
plt.ylabel('Cost')
plt.xlabel('W')
plt.show()
```

```python
import tensorflow as tf

x_data = [1., 2., 3.]
y_data = [1., 2., 3.]

W = tf.Variable(tf.random_uniform([1], -10.0, 10.0))

X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)

hypothesis = W * X
cost = tf.reduce_mean(tf.square(hypothesis - Y))

descent = W - tf.mul(0.1, tf.reduce_mean(tf.mul((tf.mul(W, X) - Y), X)))
update = W.assign(descent)

init = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init)

for step in range(20):
    sess.run(update, feed_dict={X: x_data, Y: y_data})
    print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))
```


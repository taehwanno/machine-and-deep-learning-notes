# [Linear Regression](https://www.youtube.com/watch?v=Hax03rCn3UI)

Hypothesis and Cost related to Linear Regression

## Predicting exam score: regression

"학생이 X시간을 공부했는데 Y점수를 받더라."라는 데이터를 통해 학습을 시킨다. 최종적인 목표가 0~100점 사이에 예측을 하는 것이기 때문에 supervised 머신러닝 중에서도 regression이라고 부른다. traning data를 통해 학습을 진행하면 regression 모델을 형성한다는 것을 의미한다. 학습이 끝나기 전에 7시간 학습했으면 몇 점을 받을꺼 같냐고 물었을 때 65점 정도 받겠다는 예측을 해주는 것이 linear regression이라 한다.

## Regression (data)

<p align="center">
  <img src="/assets/2.linear-regression/1.png?raw=true" alt="regression data table"/>
</p>

<p align="center">
  <img src="/assets/2.linear-regression/2.png?raw=true" alt="graph of regression data table"/>
</p>

x는 기본적인 자료 혹은 feature라 칭하며 y는 결과를 의미한다. 이를 통해 linear regression을 만든다.

## (Linear) Hypothesis

linear regression 모델을 학습할 때는 **가설**을 세울 필요가 있다. 선형적인 모델이 우리가 가지고 온 데이터에 맞을 것이다 라고 가설을 세워 검증하는 것이 linear regression 이다. 세상의 많은 것들이 선형 회귀로 설명할 수 있다.

<p align="center">
  <img src="/assets/2.linear-regression/3.png?raw=true" alt="regression data table with hypothesis"/>
</p>

데이터에 맞는 **선**을 찾는 것이 곧 학습을 하는 것이라고 생각할 수 있다.

`H(x) = Wx + b`

<p align="center">
  <img src="/assets/2.linear-regression/4.png?raw=true" alt="regression data table with hypothesis"/>
</p>

## Cost Function

<p align="center">
  <img src="/assets/2.linear-regression/5.png?raw=true" alt="cost function graph"/>
</p>

어떤 선, 모델, 가설이 우리에게 좋은가?는 실제 데이터와 가설이 나타내는 점과의 거리가 가까울수록 좋으며 멀수록 좋지 않다고 할 수 있다.

선형 회귀에서는 Cost (Lost) function 이라 부른다. 즉, 실제 데이터가 우리가 세운 가설과 얼마나 다른가를 의미한다.

`H(x) - y` 혹은 `(H(x) - y)^2`로 나타낼 수 있다. 전자는 차이값이 음, 양의 값에 따라 상쇄될 가능성이 있으므로 거의 사용되지 않으며 후자는 음, 양 상관없이 차이값을 보존해주므로 전자보다는 사용될 가능성이 더 크다.

<p align="center">
  <img src="/assets/2.linear-regression/6.png?raw=true" alt="formal cost function equation"/>
</p>

형식적으로 정의한다면 위 그림과 같이 정의할 수 있다. 즉, 각 `n`번째의 `x`값을 넣어 나온 `H(x)`값과 `y`값의 차의 제곱을 더 한 후 개수만큼 나눠 평균을 내는 것을 의미한다.

<p align="center">
  <img src="/assets/2.linear-regression/7.png?raw=true" alt="cost function about W, b"/>
</p>

결국은 cost function은 `W`와 `b`의 함수가 된다. 여기서 linear regression의 숙제는 이 값을 가장 작게 하는 것이며 가장 작게 만드는 `W`와 `b`를 구하는 것이 바로 linear regression의 학습이 된다.

## Goal: Minimize cost

<p align="center">
  <img src="/assets/2.linear-regression/8.png?raw=true" alt="Goal of regression is to find W, b that minimize cost funciton"/>
</p>

cost function의 출력값을 가장 최소화하는 `W`와 `b`를 구하는 것이 학습의 목표가 된다. 최소화하는 `W`와 `b`를 구하는 것 혹은 어떤 주어진 식에서 cost function을 최소화하는 많은 알고리즘이 존재한다.

다음 시간에는 cost를 최소화하는 방법에 대해 배운다.

# [ML lab 02 - Tensorflow로 간단한 linear regression을 구현](https://www.youtube.com/watch?v=4HrSxpi3IAM&feature=youtu.be)

`Variable`로 지정해야 나중에 모델이 업데이트를 할 수 있다.

```python
import tensorflow as tf

x_data = [1, 2, 3]  # training data
y_data = [1, 2, 3]  # traning

# Try to find values for W and b that compute y_data = W * x_data + b
# (We know that W should be 1 and b 0, but Tensorflow will figure that out for us.
W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))  # Weight
b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))

# Our hypothesis
hypothesis = W * x_data + b  # H(x) = Wx + b

# Simplified cost function
cost = tf.reduce_mean(tf.square(hypothesis - y_data))  # cost(W, b) = 1/m * (sigma(H(x_i) - y_i))^2

# Minimize (Block-box codes 18 ~ 20)
a = tf.Variable(0.1)  # Learning rate, alpha
optimizer = tf.train.GradientDescentOptimizer(a)
train = optimizer.minimize(cost)

# Before starting, initialize the variables.  We will 'run' this first.
init = tf.global_variables_initializer()

# Launch the graph.
sess = tf.Session()
sess.run(init)

# Fit the line.
for step in range(2001):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(cost), sess.run(W), sess.run(b))

# Learns best fit is W: [0.1], b: [0.3]
```

```
WARNING:tensorflow:From /Users/notaehwan/machine-and-deep-learning-notes/labs/02/0.linear-regression.py:23 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
```

다음과 같은 오류가 발생하므로 `initialize_all_variables`가 아닌 `global_variables_initializer`을 사용해서 초기화를 진행하도록하자. 그리고 `xrange`가 아닌 `range`를 사용하자.

```python
a = tf.Variable(0.1)
optimizer = tf.train.GradientDescentOptimizer(a)
train = optimizer.minimizer(cost)
```

해당 코드에서의 black-box 지점이며 `sess`을 통해 실행시 어떤 알고리즘을 통해 최적화를 진행하게 된다.

변수를 초기화하고 나서 세션을 생성하고 초기화한 `init`을 세션에 넘겨서 먼저 실행시켜줘야 한다.

위의 코드는 `placeholder`를 사용하지 않고 선형 회귀를 구현했으며 다음 코드는 `placeholder`를 활용하여 구현한 코드이다. `placeholder`를 사용할 경우 우리가 가지고 있는 모델들을 재활용할 수 있다. 즉, 학습이 끝나고 나서 원하는 입력값(`X`)을 넣어서 결과값(`Y`)을 곧바로 확인할 수 있다.

```python
import tensorflow as tf

x_data = [1, 2, 3]
y_data = [1., 2., 3.]

W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))
b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))

X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)

# Our hypothesis
hypothesis = W * X + b

# Simplified cost function
cost = tf.reduce_mean(tf.square(hypothesis - Y))

# Minimize (Block-box codes 18 ~ 20)
a = tf.Variable(0.1)
optimizer = tf.train.GradientDescentOptimizer(a)
train = optimizer.minimize(cost)

init = tf.global_variables_initializer()

# Launch the graph.
sess = tf.Session()
sess.run(init)

# Fit the line.
for step in range(2001):
    sess.run(train, feed_dict={X: x_data, Y: y_data})
    if step % 20 == 0:
        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W), sess.run(b))

# Learns best fit is W: [0.1], b: [0.3]
print(sess.run(hypothesis, feed_dict={X: 5}))
print(sess.run(hypothesis, feed_dict={X: 2.5}))
```

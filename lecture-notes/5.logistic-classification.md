# [5-1. Logistic (regression) classification](https://www.youtube.com/watch?v=PIjno6paszY&feature=youtu.be)

## Recap

가설(hypothesis)을 `H(X) = WX`와 같이 세우고 실제 데이터 값과 `H(X)`의 값의 차이, 거리를 `cost`라 한다. 학습을 한다는 것은 `cost`를 최소화하는 `W` weight를 찾아내는 것을 의미한다.

## Classification

- Spam Email Detection: Span or Ham
- Facebook feed: show or hide
- Credit Card Fraudulent Transaction detection: legitimate or fraud

regression은 숫자를 예측하는 것이었다면 classification은 특히 binary classification은 2개 중 1개를 선택하는 것을 의미한다.

## 0, 1 encoding

- Span Email Detection: Spam (1) or Ham (0)
- Facebook feed: show (1) or hide (0)
- Credit Card Fraudulent Transaction detection: legitimate (0) or fraud (1)

## Linear regression ?

<p align="center">
  <img src="/assets/5.logistic-classification/1.png?raw=true" />
</p>

Binary classification도 선형 회귀를 적용해서 풀어나갈 수 있을 것 같은데 `H(X) = WX`와 같이 가설을 세우게 되면 문제가 몇가지 발생한다.
X가 커짐에 따라 H(X)가 커지는 구조가 아니라 0, 1의 2개의 결과값밖에 없기 때문에 X의 크기에 따라 분류의 기준이 되는 X의 값이 변형될 가능성이 존재한다.
위의 그림에서 `X`값이 `50`일 때 합격한 사람(`1`)이 존재한다면 가설(`H(X)`) 그래프의 기울기가 줄어들어서 이전 그래프에서 `Y`의 값이 0, 1 사이의
0.5를 기준으로 합격인 X의 값을 계산한다면 이전에 합격한 사람이 불합격으로 분류될 가능성이 존재한다.

즉, 0과 1사이의 값만 가질 수 있는데 `H(X) = WX + b`라는 가설을 세우게 되면 0보다 훨씬 작거나 1보다 훨씬 큰 값을 가질 수 있다는 게 해당 가설의 문제점이 된다.

## Logistic Hypothesis

linear regression은 간단하긴 하지만 `[0, 1]` 사이의 값으로 압축을 시켜주는 어떤 형태의 함수가 있으면 좋겠다!

<p align="center">
  <img src="/assets/5.logistic-classification/2.png?raw=true" />
</p>

`g(z) = 1 / (1 + e^(-z))`, logistic function or sigmoid function

> `sigmoid`: Curved in two directions, like the letter "S", or the Greek character (sigma).

`g(z)`의 경우 `x`가 계속 증가하더라도 1에 가까운 값에 수렴하게 되며 감소하더라도 `0`으로 수렴하게 된다.

## Logistic Hypothesis

<p align="center">
  <img src="/assets/5.logistic-classification/3.png?raw=true" />
</p>

`z = WX`로 두고 `H(X) = g(z)`가 되어 위와 같은 식이 된다.

# [5-2. Logistic (regression) classification: cost function & gradient descent](https://www.youtube.com/watch?v=6vzchGYEJBc&feature=youtu.be)

## Cost function

<p align="center">
  <img src="/assets/5.logistic-classification/4.png?raw=true" />
</p>

signoid function을 활용하여 가설 `H(X)`를 세울 경우 지역 최솟값(local minimum)에 대한 문제가 발생하게 된다. 전체에 대한 최솟값(global minimum)이
아닌 어떤 특정한 지역에서의 최솟값을 `cost`를 최소화하는 지점으로 착각할 수 있다는 것이다. 이런 상황에서는 모델이 나쁘게 예측(prediction)하게 된다.
그러므로 선형 회귀에서 활용한 그래프를 통해 생성된 `cost` 함수와 다르게 이런 형태의 그래프에서는 경사하강법(Gradient descent algorithm)을 사용할 수 없게 된다.

## New cost function for logistic

<p align="center">
  <img src="/assets/5.logistic-classification/5.png?raw=true" />
</p>

<p align="center">
  <img src="/assets/5.logistic-classification/6.png?raw=true" />
</p>

`y`의 값이 `0`, `1`일 때 cost function의 형태가 달라지게 된다. `exponential`의 상극이 `log` 이므로 `log`를 활용한다는 것이 기본 아이디어이다.
`H(x)`나 `1 - H(x)`의 값이 `0` 혹은 `1`일 경우 0으로 수렴하거나 무한대로 발산하게 된다.

최종적으로는 다음과 같이 cost function이 정의된다.

<p align="center">
  <img src="/assets/5.logistic-classification/7.png?raw=true" />
</p>

가장 아래에 존재하는 `C(H(X), y) = -y * log(H(x)) ...`와 같이 가장 처음에 등장하는 `y`에 `-1`이 곱해진다는 것을 유의하자. (PDF 판서 오류로 강의 중 추가됨)

## Minimize cost = Gradient descent algorithm

<p align="center">
  <img src="/assets/5.logistic-classification/8.png?raw=true" />
</p>

# [ML lab 05: TensorFlow로 Logistic Classification의 구현하기](https://www.youtube.com/watch?v=t7Y9luCNzzE&feature=youtu.be)

```python
import tensorflow as tf
import numpy as np

xy = np.loadtxt('train.txt', unpack=True, dtype='float32')
x_data = xy[0:-1]
y_data = xy[-1]

X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)

W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))

# Our hypothesis
h = tf.matmul(W, X)
hypothesis = tf.div(1., 1. + tf.exp(-h))  # H(X) = 1 / (1 + (e^(-W^T * X)))
# cost function
cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))

# Minimize
a = tf.Variable(0.1)  # Learning rate, alpha
optimizer = tf.train.GradientDescentOptimizer(a)
train = optimizer.minimize(cost)

# Before starting, initialize the variables. we will 'run' this first.
init = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init)

for step in range(2001):
    sess.run(train, feed_dict={X: x_data, Y: y_data})
    if step % 20 == 0:
        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))
```

## Ask to ML

트레이닝 데이터를 통해 학습을 시켰으며 모델을 만들었다. 이 모델을 활용해서 질문(입력값)에 대한 응답(분류)을 볼 수 있다.

```python
# 상단 코드와 동일
print('---------------------------')
print(sess.run(hypothesis, feed_dict={X: [[1], [2], [2]]}) > 0.5)
print(sess.run(hypothesis, feed_dict={X: [[1], [5], [5]]}) > 0.5)

print(sess.run(hypothesis, feed_dict={X: [[1, 1], [4, 3], [3, 5]]}) > 0.5)
```
